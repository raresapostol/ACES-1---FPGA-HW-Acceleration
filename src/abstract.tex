% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

Due to the rapid evolution of digital technologies, the area of deep learning artificial neural networks has demonstrated its ability and  effectiveness in solving complex learning problems of various engineering applications. However, they require intensive CPU operations and memory bandwidth that ultimately lead the CPUs to very low performance levels with respect to execution speed. Consequently, hardware accelerators that make use of application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs) and graphic processing units (GPUs) have been selected to improve the overall performance of an ANN application. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism as well as due to their energy efficiency. 
In this paper, a review of the recent existing techniques for accelerating ANNs on FPGAs is performed. It presents the key features that are being used by the various techniques that improve the acceleration performance. The techniques presented in this paper represent the recent trends in FPGA-based accelerators of artificial neural networks. \\
\textbf{Keywords:} NN, ANN, FPGA, Machine Learning
