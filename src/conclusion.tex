% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

In recent years, the accuracy yielded by neural networks is rapidly increasing. For this reason, neural networks are widely used for various applications. However, the increasing accuracy bases on an exponential expansion of the number and size of the variables used in the network, which ultimately leads to an exponential increase of the computational load. Such computational loads may be hard to handle by classical software neural networks. As a consequence, optimized neural networks hardware accelerators have been taken in consideration. Optimization can be achieved through various methods such as algorithmic operation, quantization, pruning etc. However, the target hardware accelerators architecture also plays a role in a network design. Some hardware elements may or may not support the implementation of the optimization solutions. Also, some optimization techniques increase performance whilst reducing accuracy.

FPGAs offer a programmable logic circuit that enables a highly optimized design with a relatively low design effort and initial costs. FPGA-based hardware accelerators widely use the algorithmic optimization methods described in this paper. However, depending on the structure of the hardware accelerator, the methods may yield different performances. For example, while the GEMM algorithm is well suited for FPGAs that support batching, it is not suited for streaming architectures since multiple batches would require an enormous tile size of the buffers. When using the FFT algorithm, it is more suitable for large filter sizes.

In conclusion, implementing neural networks on FPGAs is considered a feasible solution as it provides better performances, flexible architecture options and data flow optimization options while being easily reprogrammable and having a higher power efficiency.